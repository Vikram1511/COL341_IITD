{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "#%%\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import math\n",
    "import sys\n",
    "from pandas.plotting import table\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.fftpack as fp\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from mnist import MNIST\n",
    "\n",
    "\n",
    "#one hot encoder \n",
    "def kfolds(X,Y,k):\n",
    "    n = X.shape[0]\n",
    "    m_k = round(n/k)\n",
    "    data = np.hstack((X,Y))\n",
    "    K_folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        if i < k-1:\n",
    "            K_folds.append(data[m_k*i:m_k*(i+1),:])\n",
    "        else :\n",
    "            K_folds.append(data[m_k*i:,:])\n",
    "    return K_folds\n",
    "\n",
    "\n",
    "def confusion_matrix(y_true,y_pred,onehotencoded=True):\n",
    "    if(onehotencoded==True):\n",
    "        m,k = y_true.shape\n",
    "        return np.matmul(y_true.T,y_pred).astype(int)\n",
    "\n",
    "def normalized_confusion_marix(y_true,y_pred):\n",
    "    confmatrix = confusion_matrix(y_true,y_pred)\n",
    "    confmatrix = (confmatrix/np.sum(confmatrix,axis=1).reshape((confmatrix.shape[0],1))).astype(float)\n",
    "    return confmatrix\n",
    "\n",
    "def precesion_recall(y_true,y_pred):\n",
    "    #precesion = TP/TP+FP\n",
    "    #recall = TP/TP+FN\n",
    "    confmatrix  = confusion_matrix(y_true,y_pred)\n",
    "    row_sum_confmatrix = np.sum(confmatrix,axis=0).tolist()\n",
    "    column_sum_confmatrix= np.sum(confmatrix,axis=1).tolist()\n",
    "    precesion = []\n",
    "    recall = []\n",
    "    tp = []\n",
    "    for i in range(confmatrix.shape[0]):\n",
    "        for j in range(confmatrix.shape[1]):\n",
    "            if(i==j):\n",
    "                tp.append(confmatrix[i][j])\n",
    "                precesion.append(round(float(confmatrix[i][j]/row_sum_confmatrix[i]),4))\n",
    "                recall.append(round(float(confmatrix[i][j]/column_sum_confmatrix[i]),4))\n",
    "    fp = np.subtract(np.array(row_sum_confmatrix), np.array(tp)).tolist()\n",
    "    fn =  np.subtract(np.array(column_sum_confmatrix), np.array(tp)).tolist()\n",
    "    return precesion,recall,tp,fp,fn\n",
    "\n",
    "\n",
    "def micro_scores(y_true,y_pred):\n",
    "    # avg_prec = float(np.nansum(precesion)/len(precesion))\n",
    "    # avg_rec = float(np.nansum(recall)/len(recall))\n",
    "    # f1score = 2*(avg_prec*avg_rec)/(avg_prec+avg_rec)\n",
    "    _,_,tp,fp,fn = precesion_recall(y_true,y_pred)\n",
    "    tp_sum = sum(tp)\n",
    "    tp_fp_sum = sum(tp)+sum(fp)\n",
    "    tp_fn_sum = tp_sum + sum(fn)\n",
    "    micro_avg_prec = round(float(tp_sum/tp_fp_sum),3)\n",
    "    micro_avg_rec = round(float(tp_sum/tp_fn_sum),3)\n",
    "    micro_avg_f1 = round((2*micro_avg_prec*micro_avg_rec)/(micro_avg_prec+micro_avg_rec),3)\n",
    "    return micro_avg_f1,micro_avg_prec,micro_avg_rec\n",
    "\n",
    "def macro_scores(y_true,y_pred):\n",
    "    precesion,recall,_,_,_ = precesion_recall(y_true,y_pred)\n",
    "    prec_avg =round(float(np.nansum(precesion)/(len(precesion))),3)\n",
    "    rec_avg = round(float(np.nansum(recall)/len(recall)),3)\n",
    "    macro_f1 = round((2*prec_avg*rec_avg)/(prec_avg+rec_avg),3)\n",
    "    return macro_f1,prec_avg,rec_avg\n",
    "\n",
    "def save_image(df,table_name):\n",
    "    print(df)\n",
    "    fig, ax = plt.subplots(figsize=(20, 2)) # set size frame\n",
    "    ax.xaxis.set_visible(False)  # hide the x axis\n",
    "    ax.yaxis.set_visible(False)  # hide the y axis\n",
    "    ax.set_frame_on(False)  # no visible frame, uncomment if size is ok\n",
    "    tabla = table(ax, df, loc='upper right', colWidths=[0.1]*len(df.columns))  # where df is your data frame\n",
    "    tabla.auto_set_font_size(False) # Activate set fontsize manually\n",
    "    tabla.set_fontsize(12) # if ++fontsize is necessary ++colWidths\n",
    "    tabla.scale(1.3, 1.3) # change size table\n",
    "    plt.savefig(table_name)\n",
    "\n",
    "def fft_image_data(trainx):\n",
    "    #shape of trainx = m,n\n",
    "    ## Functions to go from image to frequency-image and back\n",
    "    im2freq = lambda data: fp.rfft(fp.rfft(data, axis=0),\n",
    "                                   axis=1)\n",
    "    freq2im = lambda f: fp.irfft(fp.irfft(f, axis=1),\n",
    "                                 axis=0)\n",
    "\n",
    "    ## Read in data file and transform\n",
    "    for i in range(trainx.shape[0]):\n",
    "        curr_img = trainx[i,:].reshape((32,32))\n",
    "        data = curr_img\n",
    "\n",
    "        freq = im2freq(data)\n",
    "        back = freq2im(freq)\n",
    "        assert(np.allclose(data, back))\n",
    "\n",
    "        ## Helper functions to rescale a frequency-image to [0, 255] and save\n",
    "        remmax = lambda x: x/x.max()\n",
    "        remmin = lambda x: x - np.amin(x, axis=(0,1), keepdims=True)\n",
    "        touint8 = lambda x: (remmax(remmin(x))*(256-1e-4)).astype(int)\n",
    "        trainx[i,:] = touint8(freq).reshape((1,1024))\n",
    "    return trainx\n",
    "\n",
    "def onehotEncoder(array,k_class):\n",
    "    if(type(array)==list):\n",
    "        array = np.array(array)\n",
    "    assert len(array.shape)==1\n",
    "    onehotencoded = np.zeros((array.shape[0],k_class))\n",
    "    array = array.reshape((array.shape[0],))\n",
    "    for i in range(array.shape[0]):\n",
    "        onehotencoded[i,array[i]-1] = 1\n",
    "    return onehotencoded\n",
    "\n",
    "def sigmoid_func(X):\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "def softmax_stable(y):\n",
    "    y_pred_max = -1*np.max(y,axis=1,keepdims=True)\n",
    "    y = np.exp(y+y_pred_max)\n",
    "    column_wise_sum = np.sum(y,axis=1,keepdims=True)\n",
    "    return (y/column_wise_sum)\n",
    "\n",
    "def relu_func(X):\n",
    "        X = np.where(X<0,0,X)\n",
    "        return X\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x = np.where(x<0,0,1)\n",
    "    return x\n",
    "    \n",
    "def tanh_func(X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "def tanh_prime(X):\n",
    "    return 1-np.power(tanh_func(X),2)\n",
    "\n",
    "def sigmoid_prime(X):\n",
    "    return sigmoid_func(X)*(1-sigmoid_func(X))\n",
    "\n",
    "def softmax_prime(y):\n",
    "    return softmax_stable(y)*(1-softmax_stable(y))\n",
    "\n",
    "class neuron_layer:\n",
    "    def __init__(self,inputs_neurons,number_of_neurons):\n",
    "        self.inputs_neurons=inputs_neurons\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.dropout_mask=None\n",
    "        self.batch_normalized_layer=None\n",
    "        self.set_weights(np.sqrt(2/inputs_neurons)*np.random.uniform(-1,1,(inputs_neurons,number_of_neurons)))\n",
    "        self.set_bias(np.random.uniform(-1,1,(1,number_of_neurons)))\n",
    "\n",
    "    def set_weights(self,weights):\n",
    "        self.weights = weights\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def set_bias(self,bias):\n",
    "        self.bias = bias\n",
    "    \n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "    def layer_output(self,inputs):\n",
    "        self.layer_output_z = np.matmul(inputs,self.weights) +self.bias\n",
    "    \n",
    "    def activate_layer(self,sigmoid=False,relu=False,tanh=False,softmax=False):\n",
    "        self.activate_a=None\n",
    "        if(sigmoid==True):\n",
    "            if(self.batch_normalized_layer is not None):\n",
    "                self.activate_a = sigmoid_func(self.batch_normalized_layer)\n",
    "            else:\n",
    "                self.activate_a = sigmoid_func(self.layer_output_z)\n",
    "            # if(isoutput==False):\n",
    "            #     self.activate_a = np.hstack((np.ones((self.activate_a.shape[0],1)),self.activate_a))\n",
    "        if(relu==True):\n",
    "            if(self.batch_normalized_layer is None):\n",
    "                self.activate_a = relu_func(self.layer_output_z)\n",
    "            else:\n",
    "                self.activate_a = relu_func(self.batch_normalized_layer)\n",
    "            # if(isoutput==False):\n",
    "            #     self.activate_a = np.hstack((np.ones((self.activate_a.shape[0],1)),self.activate_a))\n",
    "        if(tanh==True):\n",
    "            if(self.batch_normalized_layer is not None):\n",
    "                self.activate_a = tanh_func(self.batch_normalized_layer)\n",
    "            else:\n",
    "                self.activate_a = tanh_func(self.layer_output_z)\n",
    "            # if(isoutput==False):\n",
    "            #     self.activate_a = np.hstack((np.ones((self.activate_a.shape[0],1)),self.activate_a))\n",
    "        if(softmax==True):\n",
    "            if(self.batch_normalized_layer is not None):\n",
    "                self.activate_a = softmax_stable(self.batch_normalized_layer)\n",
    "            else:\n",
    "                self.activate_a = softmax_stable(self.layer_output_z)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class Neural_a:\n",
    "\n",
    "    def __init__(self,trainX,trainY,n_output_neurons,n_hl_neurons,ismulti=False,activation_func=\"sigmoid\",drop_prob=0,isBatchNormalize=False):\n",
    "        assert type(n_hl_neurons)==list\n",
    "        # assert (len(trainY.shape))==2\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.batch_normalize=isBatchNormalize\n",
    "        self.drop_prob=drop_prob\n",
    "        #activation function for hidden layer, wheather sigmoid or relu or tanh\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        #numbe of hidden layers\n",
    "        self.n_layers = len(n_hl_neurons)\n",
    "\n",
    "        #number of units in output layer\n",
    "        self.n_output_neurons = n_output_neurons\n",
    "\n",
    "        #whether classification is multiclass or binary\n",
    "        self.ismulti = ismulti\n",
    "\n",
    "        #to create architecture of network\n",
    "        self.create_architechure(n_hl_neurons)\n",
    "    \n",
    "    def create_architechure(self,neurons_list):\n",
    "        if(self.ismulti==True):\n",
    "            assert self.n_output_neurons > 1\n",
    "        '''\n",
    "        neurons list is a list containing number of neurons in each hidden layer\n",
    "        length of list should be equal to number of hidden layers\n",
    "        it should not have input layer and output layer shape\n",
    "        '''\n",
    "        assert type(neurons_list)==list\n",
    "\n",
    "        #first hidden layer\n",
    "        self.layers = [neuron_layer(self.trainX.shape[1],neurons_list[0])]\n",
    "\n",
    "        #adding hidden layer upto output layer\n",
    "        self.layers =self.layers + [neuron_layer(neurons_list[i-1],neurons_list[i]) for i in range(1,len(neurons_list))]\n",
    "\n",
    "        #output layer which is last layer\n",
    "        self.layers = self.layers+ [neuron_layer(neurons_list[-1],self.n_output_neurons)]\n",
    "\n",
    "    def forward_propagate(self,x,istraining=True):\n",
    "\n",
    "        #Z1 value\n",
    "        self.layers[0].layer_output(x)\n",
    "\n",
    "        #batchnormalzing Z1 value before activation\n",
    "        if(self.batch_normalize==True):\n",
    "            self.layers[0].batch_normalized_layer = self.batch_normalization(self.layers[0].layer_output_z)\n",
    "\n",
    "        for i in range(1,len(self.layers)):\n",
    "\n",
    "            if(self.activation_func ==\"sigmoid\"):\n",
    "                #applying activation function\n",
    "                self.layers[i-1].activate_layer(sigmoid=True)\n",
    "\n",
    "                #calculating drop out mask ith drop_prob probability\n",
    "                self.layers[i-1].dropout_mask = self.dropout(self.layers[i-1].activate_a,self.drop_prob)\n",
    "            if(self.activation_func =='relu'):\n",
    "                self.layers[i-1].activate_layer(relu=True)\n",
    "                self.layers[i-1].dropout_mask = self.dropout(self.layers[i-1].activate_a,self.drop_prob)\n",
    "            if(self.activation_func =='tanh'):\n",
    "                self.layers[i-1].activate_layer(tanh=True)\n",
    "                self.layers[i-1].dropout_mask = self.dropout(self.layers[i-1].activate_a,self.drop_prob)\n",
    "            \n",
    "            #feeding next hidden layer\n",
    "            '''\n",
    "            in training phase dropout applied but in test phase it should not\n",
    "            '''\n",
    "            if(istraining==True):\n",
    "                self.layers[i].layer_output(self.layers[i-1].dropout_mask)\n",
    "\n",
    "                #batch normalization of Z values of this hidden layer\n",
    "                if(self.batch_normalize==True):\n",
    "                    self.layers[i].batch_normalized_layer = self.batch_normalization(self.layers[i].layer_output_z)\n",
    "            if(istraining==False):\n",
    "                self.layers[i].layer_output(self.layers[i-1].activate_a)\n",
    "                if(self.batch_normalize==True):\n",
    "                    self.layers[i].batch_normalized_layer = self.batch_normalization(self.layers[i].layer_output_z)\n",
    "\n",
    "        #activation of last layer \n",
    "        if(self.ismulti==True):\n",
    "            self.layers[-1].activate_layer(softmax=True)\n",
    "        else:\n",
    "            self.layers[-1].activate_layer(sigmoid=True)\n",
    "        return self.layers[-1].activate_a\n",
    "\n",
    "    def Binary_cross_entropy_loss(self,y,outputs):\n",
    "        term1 = y*np.log(outputs[:,0])\n",
    "        term2 = (1-y)*np.log(1-outputs[:,0])\n",
    "        return -(1/y.shape[0])*np.sum(term1+term2)\n",
    "    \n",
    "    def multiclass_ce_loss(self,y,outputs):\n",
    "        return -1*(np.sum(y*np.log(outputs+1e-6))/y.shape[0])\n",
    "    \n",
    "    def multiclass_ce_derivative(self,y):\n",
    "        assert self.layers[-1].activate_a.shape ==y.shape\n",
    "        return (self.layers[-1].activate_a - y)/y.shape[0]\n",
    "\n",
    "\n",
    "    #derivative of binary cross entropy cost with respect to Z values\n",
    "    def cost_derivative(self,trainy):\n",
    "        #shape of y --> m*2\n",
    "        #shape of t --> m*1\n",
    "        y  = self.layers[-1].activate_a \n",
    "        t = trainy\n",
    "        return (y-t)/trainy.shape[0]   #dl/dy\n",
    "    \n",
    "    def dropout(self,X,drop_prob):\n",
    "        assert 0<=drop_prob<=1\n",
    "        if(drop_prob==1):\n",
    "            return np.zeros_like(X)\n",
    "        mask = np.random.uniform(0,1,X.shape) > drop_prob\n",
    "        return (mask*X)/(1.0-drop_prob)\n",
    "        \n",
    "    \n",
    "    def mini_batch(self,batch_size):\n",
    "        number_of_mini_batch=math.floor(self.trainX.shape[0]/batch_size)\n",
    "        self.batches_x = []\n",
    "        self.batches_y =[]\n",
    "        for i in range(number_of_mini_batch):\n",
    "            self.batches_x.append(self.trainX[i*batch_size:(i+1)*batch_size,:])\n",
    "            self.batches_y.append(self.trainY[i*batch_size:(i+1)*batch_size,:])\n",
    "        if(number_of_mini_batch*batch_size<self.trainX.shape[0]):\n",
    "            self.batches_x.append(self.trainX[number_of_mini_batch*batch_size:,:])\n",
    "            self.batches_y.append(self.trainY[number_of_mini_batch*batch_size:,:])\n",
    "        \n",
    "        return self.batches_x,self.batches_y\n",
    "        \n",
    "        \n",
    "    def batch_normalization(self,X):\n",
    "        # scaler = StandardScaler()\n",
    "        # X  = scaler.fit_transform(X)\n",
    "        X_mu = np.mean(X,axis=0,keepdims=True)\n",
    "        X_var = np.var(X,axis=0,keepdims=True)\n",
    "        X = (X-X_mu)/np.sqrt(X_var+1e-6)\n",
    "        return X\n",
    "\n",
    "    def accuracy(self,y,outputs):\n",
    "        '''\n",
    "        y and outputs are of size m,k where k is number of classes\n",
    "        '''\n",
    "        y =np.argmax(y,axis=1)\n",
    "        outputs = np.argmax(outputs,axis=1)\n",
    "        acc = [1 if x==y else 0 for (x,y) in zip(y,outputs)]\n",
    "        return float(np.sum(acc)/len(acc))\n",
    "    \n",
    "    def predictions(self,x):\n",
    "        pred_outputs = self.forward_propagate(x)\n",
    "        return pred_outputs\n",
    "\n",
    "    \n",
    "    def back_propagate(self,x,y):\n",
    "\n",
    "        #initializing all weights to zeros\n",
    "        back_prop_weights_updates = [np.zeros((w.get_weights().shape)) for w in self.layers]\n",
    "        back_prop_bias_updates = [np.zeros(b.get_bias().shape) for b in self.layers]\n",
    "\n",
    "        #feed forward \n",
    "        self.forward_propagate(x)\n",
    "\n",
    "        #delta for output layer\n",
    "        if(self.ismulti==True):\n",
    "            delta = self.multiclass_ce_derivative(y)\n",
    "        else:\n",
    "            delta = self.cost_derivative(y)  #dl/dz\n",
    "\n",
    "        #dw and db for output layer\n",
    "        back_prop_weights_updates[-1] = np.matmul(self.layers[-2].activate_a.T,delta)\n",
    "        back_prop_bias_updates[-1] = np.sum(delta,axis=0,keepdims=True)\n",
    "\n",
    "        for i in range(1,len(self.layers)):\n",
    "            curr_layer = self.layers[-(i+1)]\n",
    "            prev_layer = self.layers[-i]\n",
    "\n",
    "            #calculation of derivative of activation function\n",
    "            if(self.activation_func=='sigmoid'):\n",
    "                sp = curr_layer.activate_a*(1-curr_layer.activate_a)\n",
    "            if(self.activation_func=='relu'):\n",
    "                sp = relu_derivative(curr_layer.layer_output_z)\n",
    "            if(self.activation_func =='tanh'):\n",
    "                sp = tanh_prime(curr_layer.layer_output_z)\n",
    "\n",
    "            #delta calculation for hidden layers\n",
    "            delta = np.matmul(delta,prev_layer.get_weights().T)*sp*(curr_layer.dropout_mask/(1.0-self.drop_prob))\n",
    "            if(i==(len(self.layers)-1)):\n",
    "                activations = x.T\n",
    "            else:\n",
    "                activations = self.layers[-(i+2)].activate_a.T\n",
    "            # print(back_prop_weights_updates[-(i+1)].shape)\n",
    "            back_prop_weights_updates[-(i+1)]= np.dot(activations,delta)\n",
    "            back_prop_bias_updates[-(i+1)]= np.sum(delta,axis=0,keepdims=True)\n",
    "        \n",
    "        return back_prop_weights_updates,back_prop_bias_updates\n",
    "\n",
    "    def train_network(self,valx,valy,learning_rate,iterations,n_batches,adaptive=False):\n",
    "        self.mini_batch(n_batches)\n",
    "        x,y = self.batches_x,self.batches_y\n",
    "        k = len(x)\n",
    "        train_cost_history=[]\n",
    "        val_cost_history=[]\n",
    "        train_acc_hist=[]\n",
    "        val_acc_hist=[]\n",
    "        epochs=0\n",
    "        for i in range(iterations):\n",
    "            # weights = [np.zeros(w.get_weights().shape) for w in self.layers]\n",
    "            # biases =  [np.zeros(w.get_bias().shape) for w in self.layers]\n",
    "            mini_batchx,mini_batchy = x[i%k],y[i%k]\n",
    "            dw,db = self.back_propagate(mini_batchx,mini_batchy)\n",
    "\n",
    "            if(adaptive==False):\n",
    "                for j,w in enumerate(dw):\n",
    "                    new_weights = self.layers[j].get_weights() - (learning_rate)*w\n",
    "                    self.layers[j].set_weights(new_weights)\n",
    "                for j,b in enumerate(db):\n",
    "                    new_biases = self.layers[j].get_bias() - (learning_rate)*b\n",
    "                    self.layers[j].set_bias(new_biases)\n",
    "            \n",
    "            if(adaptive==True):\n",
    "                for j,w in enumerate(dw):\n",
    "                    new_weights = self.layers[j].get_weights() - (learning_rate/(np.sqrt(i+1)))*w\n",
    "                    self.layers[j].set_weights(new_weights)\n",
    "                for j,b in enumerate(db):\n",
    "                    new_biases = self.layers[j].get_bias() - (learning_rate/(np.sqrt(i+1)))*b\n",
    "                    self.layers[j].set_bias(new_biases)\n",
    "            \n",
    "            if(i%len(x)==0):\n",
    "                epochs=epochs+1\n",
    "                outputs = self.forward_propagate(self.trainX,istraining=False)\n",
    "                val_outputs = self.forward_propagate(valx,istraining=False)\n",
    "                train_cost =self.multiclass_ce_loss(self.trainY,outputs)\n",
    "                val_cost  = self.multiclass_ce_loss(valy,val_outputs)\n",
    "                train_cost_history.append(train_cost)\n",
    "                val_cost_history.append(val_cost)\n",
    "    \n",
    "                train_acc = self.accuracy(self.trainY,outputs)\n",
    "                train_acc_hist.append(train_acc)\n",
    "                val_acc  =self.accuracy(valy,val_outputs)\n",
    "                val_acc_hist.append(val_acc)\n",
    "                print(\"Epoch no.: \"+ str(epochs)+\"  train_cost:\"+str(train_cost)+\"  val_cost:\"+str(val_cost)+\" train_acc:\"+str(train_acc)+\"  val_acc:\"+str(val_acc))\n",
    "            # ou = sigmoid_func(self.forward_propagate(self.trainX))\n",
    "            # print(self.Binary_cross_entropy_loss(self.trainY,ou))\n",
    "        return self.layers[-1].activate_a,train_cost_history,val_cost_history,train_acc_hist,val_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 1  train_cost:2.6083809835822844  val_cost:2.6049528220799454 train_acc:0.11125  val_acc:0.105\n",
      "Epoch no.: 2  train_cost:2.0587787006235003  val_cost:2.088701489562659 train_acc:0.2665625  val_acc:0.24775\n",
      "Epoch no.: 3  train_cost:1.9969672184672218  val_cost:2.051061399321168 train_acc:0.291  val_acc:0.27175\n",
      "Epoch no.: 4  train_cost:1.9496872618187062  val_cost:1.9999509312743295 train_acc:0.3100625  val_acc:0.28425\n",
      "Epoch no.: 5  train_cost:1.9274394145044866  val_cost:2.0174996300631034 train_acc:0.320625  val_acc:0.30075\n",
      "Epoch no.: 6  train_cost:1.8985948139024418  val_cost:1.998207366550344 train_acc:0.3280625  val_acc:0.30225\n",
      "Epoch no.: 7  train_cost:1.8760554769922801  val_cost:1.983253902383151 train_acc:0.336875  val_acc:0.29975\n",
      "Epoch no.: 8  train_cost:1.8605135364225929  val_cost:1.9737389360253048 train_acc:0.3415  val_acc:0.3175\n",
      "Epoch no.: 9  train_cost:1.8506447748157187  val_cost:1.9965498351330597 train_acc:0.3476875  val_acc:0.30075\n",
      "Epoch no.: 10  train_cost:1.8494163984341334  val_cost:2.0074434509756864 train_acc:0.350375  val_acc:0.302\n",
      "Epoch no.: 11  train_cost:1.8201070136775963  val_cost:2.0068042709616183 train_acc:0.362  val_acc:0.3135\n",
      "Epoch no.: 12  train_cost:1.8247700062095051  val_cost:2.003980956473493 train_acc:0.365375  val_acc:0.3115\n",
      "Epoch no.: 13  train_cost:1.7901950211943978  val_cost:1.999148043247193 train_acc:0.370875  val_acc:0.31375\n",
      "Epoch no.: 14  train_cost:1.8016458327075782  val_cost:2.015411659475409 train_acc:0.373875  val_acc:0.313\n",
      "Epoch no.: 15  train_cost:1.7777933389595375  val_cost:2.0105249734677852 train_acc:0.3816875  val_acc:0.32775\n",
      "Epoch no.: 16  train_cost:1.7746685001042268  val_cost:2.0365941923081623 train_acc:0.3808125  val_acc:0.31925\n",
      "Epoch no.: 17  train_cost:1.7689601300816185  val_cost:2.060567011874244 train_acc:0.3833125  val_acc:0.31625\n",
      "Epoch no.: 18  train_cost:1.7671753874236356  val_cost:2.0645890360607253 train_acc:0.389125  val_acc:0.3195\n",
      "Epoch no.: 19  train_cost:1.7470154415385548  val_cost:2.0604435400247354 train_acc:0.3949375  val_acc:0.31875\n",
      "Epoch no.: 20  train_cost:1.7417050656989537  val_cost:2.0927410672833697 train_acc:0.402125  val_acc:0.3105\n",
      "Epoch no.: 21  train_cost:1.740114984056671  val_cost:2.106340181542251 train_acc:0.4076875  val_acc:0.32325\n",
      "Epoch no.: 22  train_cost:1.7375332705022493  val_cost:2.1200369201742943 train_acc:0.402125  val_acc:0.31525\n",
      "Epoch no.: 23  train_cost:1.718664590732888  val_cost:2.095713638096576 train_acc:0.4114375  val_acc:0.327\n",
      "Epoch no.: 24  train_cost:1.730173970507935  val_cost:2.1113961821208793 train_acc:0.4078125  val_acc:0.3305\n",
      "Epoch no.: 25  train_cost:1.7253274527374178  val_cost:2.1186258887664744 train_acc:0.419875  val_acc:0.33075\n",
      "Epoch no.: 26  train_cost:1.7234877359805099  val_cost:2.1696386910322354 train_acc:0.415125  val_acc:0.318\n",
      "Epoch no.: 27  train_cost:1.7052449955391338  val_cost:2.1545750664664647 train_acc:0.4226875  val_acc:0.32325\n",
      "Epoch no.: 28  train_cost:1.7121538571059274  val_cost:2.1936809383426525 train_acc:0.4256875  val_acc:0.326\n",
      "Epoch no.: 29  train_cost:1.697149252640186  val_cost:2.1905985991888755 train_acc:0.4306875  val_acc:0.32875\n",
      "Epoch no.: 30  train_cost:1.7143878769464944  val_cost:2.2271292248496093 train_acc:0.426625  val_acc:0.3225\n",
      "Epoch no.: 31  train_cost:1.6778436184331855  val_cost:2.2275810422050477 train_acc:0.4375  val_acc:0.31575\n",
      "Epoch no.: 32  train_cost:1.6815359919219024  val_cost:2.2563603063859934 train_acc:0.43675  val_acc:0.3235\n",
      "Epoch no.: 33  train_cost:1.6737586245687723  val_cost:2.2901622130772616 train_acc:0.4438125  val_acc:0.32\n",
      "Epoch no.: 34  train_cost:1.6552230530521777  val_cost:2.2901911528072807 train_acc:0.4473125  val_acc:0.323\n",
      "Epoch no.: 35  train_cost:1.6848594937325585  val_cost:2.3122496050629477 train_acc:0.44375  val_acc:0.31825\n",
      "Epoch no.: 36  train_cost:1.6726926790569008  val_cost:2.302582854166342 train_acc:0.4464375  val_acc:0.3215\n",
      "Epoch no.: 37  train_cost:1.663048351838052  val_cost:2.3353674335267405 train_acc:0.4501875  val_acc:0.3345\n",
      "Epoch no.: 38  train_cost:1.6332576121765214  val_cost:2.3290226677585264 train_acc:0.459875  val_acc:0.32925\n",
      "Epoch no.: 39  train_cost:1.6407741946464038  val_cost:2.340018540849782 train_acc:0.463125  val_acc:0.3255\n",
      "Epoch no.: 40  train_cost:1.6630367907220622  val_cost:2.419528936215944 train_acc:0.4589375  val_acc:0.32275\n",
      "Epoch no.: 41  train_cost:1.6594256542318813  val_cost:2.388840263239933 train_acc:0.45775  val_acc:0.3205\n",
      "Epoch no.: 42  train_cost:1.633898009594557  val_cost:2.467723403937186 train_acc:0.4651875  val_acc:0.32575\n",
      "Epoch no.: 43  train_cost:1.6518234500070372  val_cost:2.459423155152452 train_acc:0.468625  val_acc:0.3295\n",
      "Epoch no.: 44  train_cost:1.6194590187065352  val_cost:2.4686164063941525 train_acc:0.4793125  val_acc:0.32925\n",
      "Epoch no.: 45  train_cost:1.6090329175230202  val_cost:2.486615560915062 train_acc:0.47975  val_acc:0.33175\n",
      "Epoch no.: 46  train_cost:1.6360045716116542  val_cost:2.5528118936836424 train_acc:0.4720625  val_acc:0.325\n",
      "Epoch no.: 47  train_cost:1.6466098517541707  val_cost:2.5190774531674607 train_acc:0.4669375  val_acc:0.31725\n",
      "Epoch no.: 48  train_cost:1.6253839811624924  val_cost:2.5693840733289877 train_acc:0.486  val_acc:0.33075\n",
      "Epoch no.: 49  train_cost:1.6100176674130755  val_cost:2.5693935140513595 train_acc:0.4838125  val_acc:0.32525\n",
      "Epoch no.: 50  train_cost:1.6557193177793523  val_cost:2.6523500049090156 train_acc:0.48525  val_acc:0.32325\n",
      "Epoch no.: 51  train_cost:1.6018364541399137  val_cost:2.6324632518498134 train_acc:0.4953125  val_acc:0.32375\n",
      "Epoch no.: 52  train_cost:1.6220790631372741  val_cost:2.6203915284592965 train_acc:0.497125  val_acc:0.3285\n",
      "Epoch no.: 53  train_cost:1.6068034939205706  val_cost:2.6805439783932665 train_acc:0.496125  val_acc:0.31575\n",
      "Epoch no.: 54  train_cost:1.5870587426002467  val_cost:2.708241444307898 train_acc:0.4978125  val_acc:0.32675\n",
      "Epoch no.: 55  train_cost:1.5875109982789293  val_cost:2.671638610467468 train_acc:0.506375  val_acc:0.336\n",
      "Epoch no.: 56  train_cost:1.5979885704346632  val_cost:2.7197843616655506 train_acc:0.507125  val_acc:0.322\n",
      "Epoch no.: 57  train_cost:1.608804177075461  val_cost:2.7664890807534324 train_acc:0.5070625  val_acc:0.32775\n",
      "Epoch no.: 58  train_cost:1.570301043940743  val_cost:2.787547286035274 train_acc:0.519125  val_acc:0.32375\n",
      "Epoch no.: 59  train_cost:1.5591286402061832  val_cost:2.778397718883696 train_acc:0.5201875  val_acc:0.31975\n",
      "Epoch no.: 60  train_cost:1.571049644667981  val_cost:2.8107681078786744 train_acc:0.5185625  val_acc:0.33025\n",
      "Epoch no.: 61  train_cost:1.6024343749905703  val_cost:2.9080296384668616 train_acc:0.5109375  val_acc:0.31925\n",
      "Epoch no.: 62  train_cost:1.5394275577754692  val_cost:2.848280247455872 train_acc:0.525  val_acc:0.32125\n",
      "Epoch no.: 63  train_cost:1.5592850724879603  val_cost:2.8730191001805374 train_acc:0.5299375  val_acc:0.34\n",
      "Epoch no.: 64  train_cost:1.5734623165408717  val_cost:2.889992155551498 train_acc:0.5206875  val_acc:0.332\n",
      "Epoch no.: 65  train_cost:1.5524964746272436  val_cost:2.9696664244154616 train_acc:0.528625  val_acc:0.32425\n",
      "Epoch no.: 66  train_cost:1.5467254514086264  val_cost:2.948614072120775 train_acc:0.5265  val_acc:0.32325\n",
      "Epoch no.: 67  train_cost:1.5623833465495474  val_cost:2.9859617089655948 train_acc:0.536125  val_acc:0.32925\n",
      "Epoch no.: 68  train_cost:1.533701080876778  val_cost:3.082935957001088 train_acc:0.5449375  val_acc:0.3255\n",
      "Epoch no.: 69  train_cost:1.5622002391628456  val_cost:3.0410544483556112 train_acc:0.5355625  val_acc:0.3315\n",
      "Epoch no.: 70  train_cost:1.537997410839807  val_cost:3.0599203692340677 train_acc:0.5415  val_acc:0.3345\n",
      "Epoch no.: 71  train_cost:1.5727861060988404  val_cost:3.196387925385725 train_acc:0.5368125  val_acc:0.3275\n",
      "Epoch no.: 72  train_cost:1.5229495569372142  val_cost:3.1198961174144793 train_acc:0.5524375  val_acc:0.32825\n",
      "Epoch no.: 73  train_cost:1.5555214323041284  val_cost:3.233602515105269 train_acc:0.5476875  val_acc:0.3245\n",
      "Epoch no.: 74  train_cost:1.577215501405558  val_cost:3.150154192767592 train_acc:0.544625  val_acc:0.3355\n",
      "Epoch no.: 75  train_cost:1.5693947966294188  val_cost:3.1544649159669307 train_acc:0.5499375  val_acc:0.32975\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "input_file = 'train.csv'\n",
    "test_file = 'test_X.csv'\n",
    "\n",
    "df = pd.read_csv(input_file,header=None,error_bad_lines = False)\n",
    "df_test = pd.read_csv(test_file,header=None,error_bad_lines = False)    \n",
    "last_column = len(df.columns)-1\n",
    "last_column_test = len(df_test.columns)-1\n",
    "trainy = df[last_column].values.reshape((df.shape[0],))\n",
    "trainx  = df.drop([last_column],axis=1).values\n",
    "trainy=trainy+1\n",
    "trainy = onehotEncoder(trainy,10)\n",
    "\n",
    "#fourier transform -> in frequency domain\n",
    "trainx = trainx/255.0\n",
    "trainx = scaler.fit_transform(trainx)\n",
    "trainx,valx,trainy,valy = train_test_split(trainx,trainy,test_size=0.2,random_state=42)\n",
    "\n",
    "#fourier transform - in frequency domain\n",
    "testx = df_test.drop([last_column_test],axis=1).values\n",
    "testx= testx/255.0\n",
    "testx = scaler.fit_transform(testx)\n",
    "\n",
    "\n",
    "neural_net = Neural_a(trainx,trainy,10,[517],ismulti=True,activation_func='relu',drop_prob=0.5,isBatchNormalize=False)\n",
    "_,train_cost_hist,val_cost_hist,train_acc_hist,val_acc_hist = neural_net.train_network(valx,valy,0.01,12000,100)\n",
    "    # outputs = neural_net.forward_propagate(testx,istraining=False)\n",
    "    # outputs = np.argmax(outputs,axis=1).tolist()\n",
    "\n",
    "# weights_file = weights_file[:-4]+str(iterations)+weights_file[-4:\n",
    "\n",
    "# with open(output_file,\"w\") as f:\n",
    "\n",
    "#     for o in outputs:\n",
    "#                 f.write(str(o))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"accuracy V/s Epochs\")\n",
    "ax.plot(train_acc_hist,label=\"drop_prob:\"+str(0.5))\n",
    "legend = ax.legend(loc='lower right', shadow=True, fontsize='x-large')\n",
    "plt.savefig(\"train_accuracy_dp05_12000.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
